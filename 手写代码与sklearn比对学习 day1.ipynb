{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载包及数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据并切分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_digits()  #加载手写数字数据集\n",
    "x = data.data  #数据\n",
    "y = data.target  #标签\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "#划分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn算法实现并封装类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化参数；  \n",
    "- fit训练模型，knn传入自身即可；\n",
    "- 预测函数，计算测试集与训练集的欧氏距离，用投票的方法，挑选出其中票数最多的距离做为最近距离；\n",
    "- 模型评价，使用预测的准确率作为评价；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knnclassifier:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return self\n",
    "\n",
    "    def predict(self, x_predict):\n",
    "        y_predict = [self.predict_(x) for x in x_predict]\n",
    "        return np.array(y_predict)\n",
    "\n",
    "    def predict_(self, x_):\n",
    "        d = [np.sqrt(np.sum((x - x_)**2)) for x in self.x]\n",
    "        nearest = np.argsort(d)\n",
    "        top_y = [self.y[i] for i in nearest[:self.k]]\n",
    "        votes = Counter(top_y)\n",
    "        return votes.most_common(1)[0][0]\n",
    "\n",
    "    def score(self, y_test, x_predict):\n",
    "        y_predict = self.predict(x_predict)\n",
    "        return sum(y_test == y_predict) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Knn=knnclassifier(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9888888888888889"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=Knn.fit(x_train,y_train)\n",
    "p=Knn.predict(x_test)\n",
    "Knn.score(y_test,x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 封装knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    sklearn.neighbors 提供了 neighbors-based (基于邻居的) 无监督学习以及监督学习方法的功能  \n",
    "    监督学习分为两种： classification （分类）针对的是具有离散标签的数据，regression （回归）针对的是具有连续标签的数据。   \n",
    "    无监督的最近邻是许多其它学习方法的基础，尤其是 manifold learning (流行学习) 和 spectral clustering (谱聚类)。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 无监督 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sklearn.neighbors.NearestNeighbors（n_neighbors = 5，radius = 1.0，algorithm ='auto'，leaf_size = 30，metric ='minkowski'，p = 2，metric_params = None，n_jobs = None，** kwargs) \n",
    "\n",
    "参数：\t  \n",
    "- n_neighbors ： int，optional（default = 5）  \n",
    "    默认情况下kneighbors查询使用的邻居数。\n",
    "\n",
    "\n",
    "- radius ： float，optional（默认值= 1.0）  \n",
    "    默认情况下用于radius_neighbors 查询的参数空间范围。\n",
    "  \n",
    "  \n",
    "- algorithm ： {'auto'，'ball_tree'，'kd_tree'，'brute'}，可选  \n",
    "    用于计算最近邻居的算法：  \n",
    "    'ball_tree' 将使用 BallTree  \n",
    "    'kd_tree' 将使用 KDTree  \n",
    "    'brute' 将使用蛮力搜索。  \n",
    "    'auto' 将尝试根据传递给fit方法的值来确定最合适的算法。  \n",
    "    注意：在稀疏输入上拟合将使用强力来覆盖此参数的设置。  \n",
    "\n",
    "\n",
    "- leaf_size ： int，optional（默认值= 30）    \n",
    "    叶子大小传递给BallTree或KDTree。这可能会影响构造和查询的速度，以及存储树所需的内存。最佳值取决于问题的性质。\n",
    "\n",
    "\n",
    "- metric ： 字符串或可调用，默认为'minkowski'    \n",
    "    用于距离计算的度量。可以使用scikit-learn或scipy.spatial.distance中的任何指标。\n",
    "    \n",
    "    \n",
    "- p ： 整数，可选（默认= 2）    \n",
    "    来自sklearn.metrics.pairwise.pairwise_distances的Minkowski度量的参数。当p = 1时，这相当于使用manhattan_distance（l1），并且对于p = 2使用euclidean_distance（l2）。对于任意p，使用minkowski_distance（l_p）。  \n",
    "\n",
    "\n",
    "- metric_params ： dict，optional（默认=无）  \n",
    "    度量函数的其他关键字参数。\n",
    "\n",
    "\n",
    "- n_jobs ： int或None，可选（默认=无）  \n",
    "    为邻居搜索运行的并行作业数。 None除非在joblib.parallel_backend上下文中，否则表示1 。 -1表示使用所有处理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [4, 3],\n",
       "       [5, 4]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices  #索引         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.41421356]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances #距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbrs.kneighbors_graph(X).toarray()\n",
    "# 样本数A [i，j]被赋予连接i到j的边的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kneighbors(self[, X, n_neighbors, …])\t找到一个点的k近邻点\n",
    "    kneighbors_graph(self[, X, n_neighbors, mode])\t计算X中点的k-邻居的（加权）图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，我们可以使用 KDTree 或 BallTree 来找最近邻。 这是上文使用过的 NearestNeighbors 类所包含的功能。 KDTree 和 BallTree 具有相同的接口 \n",
    "\n",
    "KDTree(X, leaf_size=40, metric=’minkowski’, **kwargs)\n",
    "\n",
    "BallTree(X, leaf_size=40, metric=’minkowski’, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类  \n",
    "sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)\n",
    "\n",
    "回归  \n",
    "sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)     \n",
    "二者具有相似的api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**weights ： str或callable，可选（默认='uniform'）**  \n",
    "用于预测的权重函数。可能的值：\n",
    "\n",
    "'uniform'：均匀的重量。每个社区的所有积分均等。  \n",
    "’distance'：重量点距离的倒数。在这种情况下，查询点的较近邻居将比远离的邻居具有更大的影响力。  \n",
    "[callable]：一个用户定义的函数，它接受一个距离数组，并返回一个包含权重的相同形状的数组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法|用途\n",
    "--|--\n",
    "fit(self, X, y) | 使用X作为训练数据并使用y作为目标值来拟合模型\n",
    "get_params(self[, deep]) |\t获取此估算工具的参数。\n",
    "kneighbors(self[, X, n_neighbors, …]) |\t找到一个点的K邻居。\n",
    "kneighbors_graph(self[, X, n_neighbors, mode])\t|计算X中点的k-邻居的（加权）图\n",
    "predict(self, X)\t| 预测所提供数据的类标签（分类）/预测所提供数据的目标（回归）\n",
    "predict_proba(self, X) |\t测试数据X的返回概率估计。\n",
    "score(self, X, y[, sample_weight])|\t返回给定测试数据和标签的平均精度。(分类）/返回预测的确定系数R ^ 2。（回归）\n",
    "set_params(self, \\*\\*params)|\t设置此估算器的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9888888888888889"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 对比上面手写分类算法\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf=KNeighborsClassifier(n_neighbors=4,weights='distance')\n",
    "clf.fit(x_train,y_train)\n",
    "clf.predict(x_test)\n",
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**找到最优的k值和距离参数，调参**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.9916666666666667\n",
      "best_k = 5\n"
     ]
    }
   ],
   "source": [
    "### 找到最优的k值，调参\n",
    "best_score = 0.0\n",
    "best_k = 1\n",
    "for k in range(1, 11):\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_clf.fit(x_train, y_train)\n",
    "    score = knn_clf.score(x_test, y_test)\n",
    "    if score > best_score:\n",
    "        best_k = k\n",
    "        best_score = score\n",
    "\n",
    "print('best_score =', best_score)\n",
    "print('best_k =', best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note:若找到最好值为10（在边界点），需要扩大参数搜索范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.9916666666666667\n",
      "best_k = 5\n",
      "best_method = uniform\n"
     ]
    }
   ],
   "source": [
    "#进一步找到最优的距离参数\n",
    "best_score = 0.0\n",
    "best_k = 1\n",
    "best_method = ''\n",
    "for method in ['uniform', 'distance']:\n",
    "    for k in range(1, 11):\n",
    "        knn_clf = KNeighborsClassifier(n_neighbors=k, weights=method)\n",
    "        knn_clf.fit(x_train, y_train)\n",
    "        score = knn_clf.score(x_test, y_test)\n",
    "        if score > best_score:\n",
    "            best_k = k\n",
    "            best_score = score\n",
    "            best_method = method\n",
    "print('best_score =', best_score)\n",
    "print('best_k =', best_k)\n",
    "print('best_method =', best_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用sklearn封装的 GridSearchCV 同样可以进行上述调参操作   \n",
    "\n",
    "sklearn.model_selection.GridSearchCV（estimator，param_grid，scoring = None，n_jobs = None，iid ='warn'，refit = True，cv ='warn'，verbose = 0，pre_dispatch ='2 * n_jobs'，error_score ='raise-deprecating'，return_train_score = False )\n",
    "\n",
    "[参数:](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "如果将n_jobs设置为大于1的值，则会为网格中的每个点复制数据（而不是n_jobs次）。如果单个作业花费的时间很少，则可以提高效率，但如果数据集很大且内存不足，则可能会出错。这种情况下的解决方法是设置pre_dispatch。然后，内存仅复制pre_dispatch多次。 pre_dispatch的合理值是2 * n_jobs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yep\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'weights': ['uniform'], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, {'weights': ['distance'], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'p': [1, 2, 3, 4, 5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{\n",
    "    'weights': ['uniform'],\n",
    "    'n_neighbors': [i for i in range(1, 11)]\n",
    "}, {\n",
    "    'weights': ['distance'],\n",
    "    'n_neighbors': [i for i in range(1, 11)],\n",
    "    'p': [i for i in range(1, 6)]\n",
    "}]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(knn_clf, param_grid)\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9853862212943633"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近邻回归是用在数据标签为连续变量，而不是离散变量的情况下。分配给查询点的标签是由它的最近邻标签的均值计算而来的。\n",
    "\n",
    "scikit-learn 实现了两种不同的最近邻回归：[KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) 基于每个查询点的 k 个最近邻实现， 其中 k 是用户指定的整数值。[RadiusNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor) 基于每个查询点的固定半径 r 内的邻点数量实现， 其中 r 是用户指定的浮点数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后续内容参阅   \n",
    "[算法选择](http://sklearn.apachecn.org/#/docs/7?id=_1644-%e6%9c%80%e8%bf%91%e9%82%bb%e7%ae%97%e6%b3%95%e7%9a%84%e9%80%89%e6%8b%a9)    \n",
    "[leaf_size-的影响](http://sklearn.apachecn.org/#/docs/7?id=_1645-leaf_size-%e7%9a%84%e5%bd%b1%e5%93%8d)   \n",
    "[邻域成分分析](http://sklearn.apachecn.org/#/docs/7?id=_166-%e9%82%bb%e5%9f%9f%e6%88%90%e5%88%86%e5%88%86%e6%9e%90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "手写代码与sklearn比对学习 day1.ipynb",
    "public": true
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "261px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
